{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Aryaman Pandya \n",
    "Sequential Machine Learning \n",
    "Building a Vanilla RNN \n",
    "Model and trainer implementation \n",
    "Following https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_agnews.ipynb\n",
    "implementation minus the memory unit for now \n",
    "'''\n",
    "import torch \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import plotly\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#Class definition of Vanilla RNN \n",
    "class VanillaRNN(nn.Module): \n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_len, num_layers) -> None:\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.hidden_size = hidden_size \n",
    "        self.output_len = output_len \n",
    "        \n",
    "        self.rnn = nn.RNN(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, dropout=0.5,\n",
    "                                batch_first=True, bidirectional=True) #graph module to compute next hidden state \n",
    "        \n",
    "        self.hidden2label = nn.Linear(2*hidden_size, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropoutLayer = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.encoder(x)\n",
    "        output, hidden = self.rnn(embedded)  # Pass the initial hidden state 'h' to the RNN\n",
    "        \n",
    "        \n",
    "        # Flatten the output tensor to match the linear layer input size\n",
    "        output = output.contiguous().view(-1, 2 * self.hidden_size)\n",
    "        \n",
    "        # Apply dropout to the concatenated hidden state\n",
    "        hidden = self.dropoutLayer(hidden)\n",
    "        \n",
    "        # Linear layer and softmax\n",
    "        label_space = self.hidden2label(hidden)\n",
    "        output_probs = self.softmax(label_space)\n",
    "        \n",
    "        return output_probs\n",
    "\n",
    "    def init_h(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter, tokenizer):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = (AG_NEWS(split=\"train\"))\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"], max_tokens=VOCAB_SIZE)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "#train_loader = DataLoader(train_iter, batch_size = 8, shuffle = True, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wreckage']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_tokens([4999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    \n",
    "    # Sort the batch in the descending order\n",
    "    batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "        \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.int64)\n",
    "    \n",
    "    # Pad sequences\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "    \n",
    "    return label_list.to(device), text_list.to(device), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_iter, batch_size = 8, shuffle = True, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Inspect the shape of the input data\n",
    "input_data = batch[1]  # Assuming the input data is the first element of the batch\n",
    "input_shape = input_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 0, 2, 2, 3, 1, 1, 0])\n",
      "tensor([[1700,    3, 4593, 1729,  155,    4, 1608,  812, 1700,    8, 4593, 1053,\n",
      "            5,  194,  155,    4, 1608, 3948,  812, 4049,    2,  214,    4,    0,\n",
      "          285, 1061,    4,  919,   11,   20,  201,  206,  366,  299,    1,   41,\n",
      "         1465, 1542,    3, 1177, 1429, 1547,    3, 1539,    3, 1543, 2715, 1540,\n",
      "           41,  164,    0,  163,   41, 1532],\n",
      "        [   0,   16, 1933,    0, 1610,  351,    3, 1407,   15,    5,   98,    6,\n",
      "            0,   21,  607,   30, 4156,  727,    4,  203,   22,  541, 2024, 2820,\n",
      "           15,  664,    4,    2, 1731,    1,    0,    0,    0, 1923,   10,   40,\n",
      "            0,    6,  100,    8,   22, 2396,    1,    1,    1,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,  153, 1101,   10, 3560,  124, 4529,   13,   27,   14,   15,\n",
      "         1146,  992,   41,  164,    1,    1,    1,   41,  163, 4749,  486,   11,\n",
      "         3560, 1962,  747,    0, 1049,   30,    0,  782,  106,  338,    7, 3416,\n",
      "          454,  153,    4,    0,   82, 2385,    0,   13,    0,    1,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [1416,    0,   22,  249,  113,   66, 1362, 1369,   16,    9,  347,  117,\n",
      "           16,    9,  482,  342, 1850,    0,   43,   24,  623,    1,  561,  106,\n",
      "            7, 1232, 3147,   18,    5,   71,  758,    3, 1395, 4931, 1473,  299,\n",
      "            6, 1193,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [4734,   93,  744,  148,    4,  376,    0,   23,   73,   15,  133, 4734,\n",
      "           33,   37, 1226,    4,    0,    2,  224,    0,   10,    0, 1653,   24,\n",
      "            0,  254,   29, 1050,  218,    1,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 211,  699,    0,    4, 4763,   47,   20,    0,    0,  211,  788, 1057,\n",
      "         4631, 1862,  192,    2,    0,    8, 1116,    4, 4763,    7,    2,  191,\n",
      "            0,   63, 1911, 2182,    7,    0,    1,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   0,  571,    0,  276, 1811, 4808,    0, 3655,    5,    0, 2786,    4,\n",
      "          578, 1045,   90,    0,    4,    2,  313, 1564,    0,  276,  707,   10,\n",
      "           55,  115,    7,  351,    1,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 132,  366, 2585,  132,  366,    6,    2,  753,   33,  675,  810,  510,\n",
      "           16,    9,    0,    0,    8,    0,    0,  363,   11,  276,    7,    0,\n",
      "            1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([54, 45, 46, 39, 30, 31, 29, 25])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(batch[0])\n",
    "print(batch[1])\n",
    "batch[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 50\n",
    "DROPOUT = 0.5\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "BIDIRECTIONAL = True\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_DIM = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VanillaRNN(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, NUM_LAYERS)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_function, optim, epochs):\n",
    "    losses = [] #group losses for loss visualization \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch %d / %d\" % (epoch+1, epochs))\n",
    "        print(\"-\"*10)\n",
    "        model.train()\n",
    "    \n",
    "        for i, batch_data in enumerate(train_loader):\n",
    "            \n",
    "            (y, x, x_size) = batch_data\n",
    "            print(\"Labels: {}, data: {}, x_size.cpu(): {}\".format(batch_data[0], x.shape,x_size.cpu()))\n",
    "            h_s = model.init_h() #initialize hidden state \n",
    "            x_packed = nn.utils.rnn.pack_padded_sequence(x, x_size.cpu(), batch_first=True)\n",
    "\n",
    "            logits = model(x, x_size)\n",
    "            loss = loss_function(logits, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss)\n",
    "\n",
    "            if(i % 10):\n",
    "              print(\"Step: {}/{}, current Epoch loss: {:.4f}\".format(i, len(train_loader), loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 50\n",
      "----------\n",
      "tensor([[ 853, 2106,    0,   59,    0,    3,  891,  402, 2729,  351,    3, 1147,\n",
      "           13,   31,   14,   53,   11,  204, 1236,    3,    2, 1908,    0,    8,\n",
      "            0,    3, 2613,    5,    0,    0,    1,  853, 2106, 4818,  435,    8,\n",
      "            0,    4,  290,   32,    0,    1,    5,   85,   16,    9, 1039,    6,\n",
      "         2896,    7, 3260,    0,   38,   66,    2, 3070,   55,  220,    5,    0,\n",
      "            3,    0, 2434,    1, 2106,   35, 1226,    4, 1608,   59,    2,    0,\n",
      "            8,  109,    5, 2729,  707,   10,  237, 2879,    3,    8,    0,  313,\n",
      "          276,    0,    0,    0, 2034, 1323,    3,  389,    4,    2,    0,    6,\n",
      "            2, 1908,    1],\n",
      "        [ 552, 1761,   51,    1,    9,    1,    4, 1096, 1443, 4143,  381,   13,\n",
      "           31,   14,   31,   15,   30,  476,    0,  820,   18,    0,  208, 4584,\n",
      "           20, 1443, 4143, 1025,   26,   55,   49,   33, 4916, 1208,    4,  123,\n",
      "            0,    3,    0,   49, 1707,    2,  689,    8,  603,   49,    0, 3387,\n",
      "           11,   32, 4360,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 366,    6,  131,  630,  210,  288,    7,  193,   42,    0,    2,   31,\n",
      "           21, 2469,   17,    2,  657,   28,  601,  303,  887,    7,  193,    3,\n",
      "          345,    2,    0,    6,    2,  539,    0,    0,    3,   17,  366,    6,\n",
      "            2,  131,  441,  947,  210,  288,    7,    2,  182,   42,  657, 2929,\n",
      "            1,    2,   31,    1,    1,    1,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 743, 2047, 2312,    0,   12,    9,   98,  153,  654,    2,  242,    0,\n",
      "            8,    0,    3,    0,   12,    9,   98,    3,   28,  134,    5, 3611,\n",
      "         1400,    6,    0,   82,  103,   28,   81, 4378, 2178,   24,    2,  743,\n",
      "            6,   22,  424,    0,    8,    0,  133,    1,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [  58,  355, 4648,   19,   70,  618,    0,  236,   13,   27,   14,   15,\n",
      "          316,   58,   92,    0,    4,  977, 1723,   10,   65, 2217,   51,    1,\n",
      "            9,    1,  452,  414,    4, 4648,    5,  715,    3, 2684,   24,    0,\n",
      "          618,    7,   70,    8,    0,  486,  250,   29,  117,    8,  196,    1,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 168,  278, 2599,    0, 1361, 1286, 2032,    3, 3836,   13,   31,   14,\n",
      "           53,  168, 1778, 1011,   26,   55,   67,   39, 1642,   40, 1286,    6,\n",
      "            5,  121,  230,   77,   87,    1,   13, 2770,   14,    8,  101, 1361,\n",
      "           64,    1,    1,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [2517, 3805, 2599,    0,  418, 2184, 2059, 1346,  943,   43,   11,    0,\n",
      "         1924, 1394,    1,    1,    1,    0,    0,    0, 2015, 3700,    1,    1,\n",
      "            1, 1087,    0,  121,   18,  547,  320,    1,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [3702, 1374,   11,    0,    2,    0,  388,    6, 3702,   21,    5,  242,\n",
      "          359,    6, 1164,    2,    0,  119,   28,  601,  559,    5, 1198, 1556,\n",
      "            0,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0]])\n",
      "Labels: tensor([0, 0, 3, 2, 2, 3, 3, 2]), data: torch.Size([8, 99]), x_size.cpu(): tensor([99, 52, 54, 44, 48, 40, 32, 26])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryaman.pandya/ml_accel/lib/python3.8/site-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "`lengths` array must be sorted in decreasing order when `enforce_sorted` is True. You can pass `enforce_sorted=False` to pack_padded_sequence and/or pack_sequence to sidestep this requirement if you do not need ONNX exportability.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model, train_loader, torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mcross_entropy, optimizer, NUM_EPOCHS)\n",
      "Cell \u001b[0;32mIn[58], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, loss_function, optim, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLabels: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, data: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, x_size.cpu(): \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(batch_data[\u001b[39m0\u001b[39m], x\u001b[39m.\u001b[39mshape,x_size\u001b[39m.\u001b[39mcpu()))\n\u001b[1;32m     14\u001b[0m h_s \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minit_h() \u001b[39m#initialize hidden state \u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m x_packed \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mrnn\u001b[39m.\u001b[39;49mpack_padded_sequence(x, x_size\u001b[39m.\u001b[39;49mcpu(), batch_first\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     17\u001b[0m logits \u001b[39m=\u001b[39m model(x, x_size)\n\u001b[1;32m     18\u001b[0m loss \u001b[39m=\u001b[39m loss_function(logits, y)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.8/site-packages/torch/nn/utils/rnn.py:263\u001b[0m, in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[1;32m    259\u001b[0m     batch_dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m batch_first \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m    260\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mindex_select(batch_dim, sorted_indices)\n\u001b[1;32m    262\u001b[0m data, batch_sizes \u001b[39m=\u001b[39m \\\n\u001b[0;32m--> 263\u001b[0m     _VF\u001b[39m.\u001b[39;49m_pack_padded_sequence(\u001b[39minput\u001b[39;49m, lengths, batch_first)\n\u001b[1;32m    264\u001b[0m \u001b[39mreturn\u001b[39;00m _packed_sequence_init(data, batch_sizes, sorted_indices, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: `lengths` array must be sorted in decreasing order when `enforce_sorted` is True. You can pass `enforce_sorted=False` to pack_padded_sequence and/or pack_sequence to sidestep this requirement if you do not need ONNX exportability."
     ]
    }
   ],
   "source": [
    "train(model, train_loader, torch.nn.functional.cross_entropy, optimizer, NUM_EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
