{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long short term memory\n",
    "\n",
    "We previously explored RNNs, neural networks that are able to propagate some hidden state through a rolled out version of itself. A major problem with RNNs is exploding or vanishing gradients. Gradient clipping solves the exploding gradient problem, but the vanishing gradient problem is harder to solve. LSTMs propose a different architecture which benefits from a hidden state like RNNs, but mitigates the vanishing gradient problem. Another issue RNNs have is that the hidden state often forgets information from a while ago in the sequence and is more biased towards more recent tokens. LSTMs also address this issue with their gated structure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch \n",
    "from torch.nn.utils.rnn import PackedSequence\n",
    "import torch.nn.functional\n",
    "from torch.utils.data import random_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model defintion\n",
    "The goal here is to build a substitute for the torch.nn.rnn.lstm module. This should be able to handle packed sequences and batched data the same way the source code for that module does. For now, does not need to have multiple layers or be bidirectional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module): \n",
    "    def __init__(self, input_size, hidden_dim, output_dim=1) -> None:\n",
    "        super().__init__()\n",
    "        self.input_dim = input_size\n",
    "        self.hidden_dim  = hidden_dim\n",
    "        \n",
    "        self.forget_gate = nn.Sequential(\n",
    "            nn.Linear(input_size+hidden_dim, hidden_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.input_gate = nn.Sequential(\n",
    "            nn.Linear(input_size+hidden_dim, hidden_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.input_node = nn.Sequential(\n",
    "            nn.Linear(input_size+hidden_dim, hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.output_gate = nn.Sequential(\n",
    "            nn.Linear(input_size+hidden_dim, hidden_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # this output layer can be fancier if needed by the use case\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h_in=None, c_in=None):\n",
    "\n",
    "        if isinstance(x, PackedSequence):\n",
    "            input, batch_sizes, sorted_indices, unsorted_indices = x\n",
    "            max_batch_size = batch_sizes[0]\n",
    "            if h_in is None: \n",
    "                h_in = self.init_h(max_batch_size, x)\n",
    "                c_in = self.init_h(max_batch_size, x)\n",
    "        \n",
    "        data_offset = 0\n",
    "        outputs = []\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"original input shape: {input.shape}\")\n",
    "            current_input = input[data_offset:data_offset + batch_size]\n",
    "            data_offset += batch_size\n",
    "            print(f\"batch_size: {batch_size}\")\n",
    "            active_h = h_in[:,:batch_size,:]\n",
    "            active_c = c_in[:,:batch_size,:]\n",
    "            print(f\"hidden_shape = {active_h.shape}\")\n",
    "            print(f\"current input size = {current_input.shape}\")\n",
    "            combined = torch.cat([current_input.unsqueeze(0), active_h], dim=2)\n",
    "            print(f\"combined shape = {combined.shape}\")\n",
    "            \n",
    "            i_gate_output = self.input_gate(combined)\n",
    "            i_node_output = self.input_node(combined)\n",
    "            o_gate_output = self.output_gate(combined)\n",
    "            f_gate_output = self.forget_gate(combined)\n",
    "\n",
    "            print(type(f_gate_output * active_c))\n",
    "            c_out = (f_gate_output * active_c) + (i_node_output * i_gate_output)\n",
    "\n",
    "            h_out = self.tanh(c_out) * o_gate_output\n",
    "            out = self.output_layer(h_out)\n",
    "\n",
    "            h_in[:batch_size] = h_out\n",
    "            c_in[:batch_size] = c_out\n",
    "            outputs.append(out)\n",
    "\n",
    "            # Handle decreasing batch size\n",
    "            h_in[batch_size:] = 0\n",
    "            c_in[batch_size:] = 0\n",
    "\n",
    "       \n",
    "        if isinstance(x, PackedSequence):\n",
    "            output_packed = PackedSequence(outputs, batch_sizes, sorted_indices, unsorted_indices)\n",
    "            return output_packed, h_out, c_out\n",
    "        \n",
    "        return out, h_out, c_out\n",
    "    \n",
    "    \n",
    "    def init_h(self, batch_size, x):\n",
    "        #alternatives include but not limited to Xavier/Kaiminh initialization\n",
    "        return torch.zeros(1, batch_size, self.hidden_dim, dtype=x.data.dtype, device=x.data.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class newsLSTM(nn.Module): \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size) -> None:\n",
    "        super(newsLSTM, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.hidden_size = hidden_size \n",
    "        \n",
    "        self.lstm = lstm(input_size=embed_size, hidden_dim=hidden_size)\n",
    "        \n",
    "        self.hidden2label = nn.Linear(2*hidden_size, 4)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropoutLayer = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, x_len):\n",
    "        embedded = self.encoder(x)\n",
    "        x_packed = nn.utils.rnn.pack_padded_sequence(embedded, x_len, batch_first=True, enforce_sorted=False)\n",
    "        output, h_t, c_t = self.lstm(x_packed)  # Pass the initial hidden state 'h' to the RNN\n",
    "        print(h_t.shape)\n",
    "        \n",
    "        hidden = self.dropoutLayer(torch.cat((h_t[-2,:,:], h_t[-1,:,:]), dim=1))\n",
    "        \n",
    "        # Linear layer and softmax\n",
    "        label_space = self.hidden2label(hidden)\n",
    "        \n",
    "        return label_space\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the exact same news classification task performed by in the bidirectionalRNN subdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "# Convert to list to enable random splitting\n",
    "train_dataset = list(train_iter)\n",
    "\n",
    "#80-20 train-val split \n",
    "train_size = int(len(train_dataset) * 0.8)  \n",
    "val_size = len(train_dataset) - train_size  \n",
    "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "\n",
    "# Build vocab based on the train_data\n",
    "train_data_iter = (text for _, text in train_data)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data_iter), specials=[\"<unk>\"], max_tokens=VOCAB_SIZE)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'Bush Courts Pa. Swing Voters on Economy (AP) AP - President Bush wooed suburban swing voters Thursday with hopeful words about the economy, contending his administration is making progress for American workers and portraying rival John Kerry as a tax-and-spend Democrat.')"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2102, 1693, 0, 0]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['word', 'probably', 'unknown', 'gibberish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maine']"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_tokens([4999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    device = torch.device(\"cpu\")\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    \n",
    "    # Sort the batch in the descending order\n",
    "    batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "        \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.int64)\n",
    "    \n",
    "    # Pad sequences\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "    \n",
    "    return label_list.to(device), text_list.to(device), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = 8, shuffle = True, collate_fn = collate_batch)\n",
    "val_loader = DataLoader(val_data, batch_size = 8, shuffle = False, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Inspect the shape of the input data\n",
    "input_data = batch[1]  # Assuming the input data is the first element of the batch\n",
    "input_shape = input_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 3, 0, 1, 3, 0, 2, 1]),\n",
       " tensor([[1059,    0,    7, 4455,    6, 3730,  110,   13,   31,   14,   31,   15,\n",
       "            25, 1826,   17, 2571, 1059,   33,   39,    4,   37, 1527,   18,  797,\n",
       "           403,   52,   71,    1,    2,   50,   16,    9,   77,    1,  108,  433,\n",
       "            86,  197,    0,  133,   20,  607, 2855,    3, 2133,   30,    0, 2335,\n",
       "            17,  291,  285,  473,    0,  641,    0, 1213, 1084,    8, 4994,    0,\n",
       "          2229,   66,   94,   16,    9,  211,  429,    6,    2,  723,  423,    1],\n",
       "         [ 117, 3048,  986,   20,    9,    1, 2357,    1, 1592,   13, 3804,    1,\n",
       "           171,   14, 3804,    1,  171,   15,    5,   97,  449,    0, 1216,  210,\n",
       "             2,  605,    3,  466,  580,    1,  248,   81,  233,   55,   11,    5,\n",
       "             0,    6,    0,   46, 2844, 1437, 3048,    0, 1085,   11,  117,  138,\n",
       "          4978,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 543,  948,    0,    4,  659,   23,  316, 1744,    2,  101,  132,   65,\n",
       "           156,    4, 3918,    5, 3325, 3441,    6,   22,  543,  948,    3,    0,\n",
       "           194,   12,    9, 1657,  358,    4,  631, 4293,   23, 1744,  555,   19,\n",
       "          1094,    8, 1088,  242,    5, 2427,  316, 1118,    1,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [3387, 1348,   40,  451, 1928, 2272,  227, 1697,   15,    2,  227, 1697,\n",
       "          3387,  591,    5, 1928, 2272,   58,    3,    2,  734,  133,   67, 2323,\n",
       "             0,    0,   29,    2,    0,    1,  222,   67,   79,    0,    2,  123,\n",
       "            11,    0,  181,  168,    2,  726,  635,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 290, 1448,   44, 2043,   44,    0,    0,  290,   28, 1103,    3,  182,\n",
       "            25,  649,    2, 2043,    3,  143,    0,    0,    0,  822,   11,  865,\n",
       "             8, 2763,  513,    1,    2,  312,    0, 1456,    5,    0, 1224, 4594,\n",
       "          4078,    0,    0,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 430,    6,  681,  387,  774,  781,   23, 4180,  430,    6,    2,    0,\n",
       "           166,  275,   93,  729,    2, 4428,   12,    9,   23, 4180,    7, 3082,\n",
       "             3,  601, 2227,   95,  443,    2,  149,   12,    9, 1727,  485,    1,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [1925,    4,  363, 1780,  421,  526,    2, 2041, 1113,  221,    3,    0,\n",
       "             7, 1128,    0,    3,   28, 1993,   92,    0,    0,   11,    0,   22,\n",
       "           421, 2486,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [  25,   16,    9,    0,    4,    0,    0,    0,    0,    3,    0, 1469,\n",
       "            60,  114,  115,   16,    9,   47, 1461,    1,   17,   16,    9,   62,\n",
       "           821,    1,    0,    3,   62,  821,   16, 3888,    1,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]),\n",
       " tensor([72, 50, 45, 43, 40, 36, 27, 33]))"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 72])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =  torch.ones(5, 50)\n",
    "a =  torch.ones(5, 50)\n",
    "a =  torch.ones(5, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 3, 0, 1, 3, 0, 2, 1])\n",
      "torch.Size([8, 72])\n"
     ]
    }
   ],
   "source": [
    "print(batch[0])\n",
    "print(batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 50\n",
    "DROPOUT = 0.5\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "BIDIRECTIONAL = True\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_DIM = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = newsLSTM(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, loss_function, optim, epochs, device):\n",
    "    losses = [] #group losses for loss visualization \n",
    "    running_loss = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(\"Epoch %d / %d\" % (epoch+1, epochs))\n",
    "        print(\"-\"*10)\n",
    "    \n",
    "        for i, batch_data in enumerate(train_loader):\n",
    "            \n",
    "            model.train()\n",
    "            (y, x, x_size) = batch_data\n",
    "            #print(\"Labels: {}, data: {}, x_size.cpu(): {}\".format(batch_data[0], x.shape,x_size.cpu()))\n",
    "\n",
    "            logits = model(x, x_size.cpu())\n",
    "            #print(\"Target size: {}, pred_size: {}\".format(y.size(), logits.size()))\n",
    "            loss = loss_function(logits, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            losses.append(loss)\n",
    "\n",
    "            if (i+1) % 1000 == 0:\n",
    "                print(\"Step: {}, average training loss over last 2000 steps: {:.4f}\".format(i+1, running_loss/1000))\n",
    "                running_loss = 0.0\n",
    "            \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, batch_data in enumerate(val_loader):\n",
    "                (y, x, x_size) = batch_data\n",
    "                y, x, x_size = y.to(device), x.to(device), x_size.to(device)\n",
    "                \n",
    "                logits = model(x, x_size.cpu())\n",
    "                loss = loss_function(logits, y)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(\"Epoch: {}, validation loss: {:.4f}\".format(epoch+1, val_loss/len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 50\n",
      "----------\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 8\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([8, 128])\n",
      "combined shape = torch.Size([1, 8, 256])\n",
      "<class 'torch.Tensor'>\n",
      "original input shape: torch.Size([422, 128])\n",
      "batch_size: 7\n",
      "hidden_shape = torch.Size([1, 8, 128])\n",
      "current input size = torch.Size([7, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 7 but got size 8 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[295], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model, train_loader, val_loader, torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mcross_entropy, optimizer, NUM_EPOCHS, DEVICE)\n",
      "Cell \u001b[0;32mIn[294], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, loss_function, optim, epochs, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m (y, x, x_size) \u001b[39m=\u001b[39m batch_data\n\u001b[1;32m     13\u001b[0m \u001b[39m#print(\"Labels: {}, data: {}, x_size.cpu(): {}\".format(batch_data[0], x.shape,x_size.cpu()))\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m logits \u001b[39m=\u001b[39m model(x, x_size\u001b[39m.\u001b[39;49mcpu())\n\u001b[1;32m     16\u001b[0m \u001b[39m#print(\"Target size: {}, pred_size: {}\".format(y.size(), logits.size()))\u001b[39;00m\n\u001b[1;32m     17\u001b[0m loss \u001b[39m=\u001b[39m loss_function(logits, y)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[277], line 17\u001b[0m, in \u001b[0;36mnewsLSTM.forward\u001b[0;34m(self, x, x_len)\u001b[0m\n\u001b[1;32m     15\u001b[0m embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x)\n\u001b[1;32m     16\u001b[0m x_packed \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpack_padded_sequence(embedded, x_len, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, enforce_sorted\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 17\u001b[0m output, h_t, c_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x_packed)  \u001b[39m# Pass the initial hidden state 'h' to the RNN\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(h_t\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     20\u001b[0m hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropoutLayer(torch\u001b[39m.\u001b[39mcat((h_t[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m,:,:], h_t[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:,:]), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[276], line 48\u001b[0m, in \u001b[0;36mlstm.forward\u001b[0;34m(self, x, h_in, c_in)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhidden_shape = \u001b[39m\u001b[39m{\u001b[39;00mactive_h\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcurrent input size = \u001b[39m\u001b[39m{\u001b[39;00mcurrent_input\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m combined \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([current_input\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m), active_h], dim\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     49\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcombined shape = \u001b[39m\u001b[39m{\u001b[39;00mcombined\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m i_gate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_gate(combined)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 7 but got size 8 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, torch.nn.functional.cross_entropy, optimizer, NUM_EPOCHS, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
