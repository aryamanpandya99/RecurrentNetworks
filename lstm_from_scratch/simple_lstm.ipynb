{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long short term memory\n",
    "\n",
    "We previously explored RNNs, neural networks that are able to propagate some hidden state through a rolled out version of itself. A major problem with RNNs is exploding or vanishing gradients. Gradient clipping solves the exploding gradient problem, but the vanishing gradient problem is harder to solve. LSTMs propose a different architecture which benefits from a hidden state like RNNs, but mitigates the vanishing gradient problem. Another issue RNNs have is that the hidden state often forgets information from a while ago in the sequence and is more biased towards more recent tokens. LSTMs also address this issue with their gated structure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch \n",
    "from torch.nn.utils.rnn import PackedSequence\n",
    "import torch.nn.functional\n",
    "from torch.utils.data import random_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model defintion\n",
    "The goal here is to build a substitute for the torch.nn.rnn.lstm module. This should be able to handle packed sequences and batched data the same way the source code for that module does. For now, does not need to have multiple layers or be bidirectional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module): \n",
    "    def __init__(self, input_size, hidden_dim, output_dim=1) -> None:\n",
    "        super().__init__()\n",
    "        self.input_dim = input_size\n",
    "        self.hidden_dim  = hidden_dim\n",
    "        \n",
    "        self.forget_gate = nn.Sequential(\n",
    "            nn.Linear(input_size+hidden_dim, hidden_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.input_gate = nn.Sequential(\n",
    "            nn.Linear(input_size+hidden_dim, hidden_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.input_node = nn.Sequential(\n",
    "            nn.Linear(input_size+hidden_dim, hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.output_gate = nn.Sequential(\n",
    "            nn.Linear(input_size+hidden_dim, hidden_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # this output layer can be fancier if needed by the use case\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h_in=None, c_in=None):\n",
    "\n",
    "        if isinstance(x, PackedSequence):\n",
    "            input, batch_sizes, sorted_indices, unsorted_indices = x\n",
    "            max_batch_size = batch_sizes[0]\n",
    "            if h_in is None: \n",
    "                h_in = self.init_h(max_batch_size, x)\n",
    "                c_in = self.init_h(max_batch_size, x)\n",
    "        \n",
    "        data_offset = 0\n",
    "        outputs = []\n",
    "        for batch_size in batch_sizes:\n",
    "            current_input = input[data_offset:data_offset + batch_size]\n",
    "            data_offset += batch_size\n",
    "            current_input = current_input.unsqueeze(0)\n",
    "            if batch_size < max_batch_size: \n",
    "                h_in[:,batch_size:,:] = 0\n",
    "                c_in[:,batch_size:,:] = 0\n",
    "                pad_size = max_batch_size - batch_size\n",
    "                # Create a tensor of zeros with the padding size\n",
    "                padding = torch.zeros(1, pad_size, self.hidden_dim, device=current_input.device)\n",
    "                # Concatenate the padding to the current input\n",
    "                current_input = torch.cat([current_input, padding], dim=1)\n",
    "            \n",
    "            combined = torch.cat([current_input, h_in], dim=2)\n",
    "            \n",
    "            i_gate_output = self.input_gate(combined)\n",
    "            i_node_output = self.input_node(combined)\n",
    "            o_gate_output = self.output_gate(combined)\n",
    "            f_gate_output = self.forget_gate(combined)\n",
    "\n",
    "            c_out = (f_gate_output * c_in) + (i_node_output * i_gate_output)\n",
    "\n",
    "            h_out = self.tanh(c_out) * o_gate_output\n",
    "            out = self.output_layer(h_out)\n",
    "            h_in = h_out\n",
    "            c_in = c_out\n",
    "            outputs.append(out)\n",
    "\n",
    "            # Handle decreasing batch siz\n",
    "\n",
    "       \n",
    "        if isinstance(x, PackedSequence):\n",
    "            output_packed = PackedSequence(outputs, batch_sizes, sorted_indices, unsorted_indices)\n",
    "            return output_packed, h_out, c_out\n",
    "        \n",
    "        return out, h_out, c_out\n",
    "    \n",
    "    \n",
    "    def init_h(self, batch_size, x):\n",
    "        #alternatives include but not limited to Xavier/Kaiminh initialization\n",
    "        return torch.zeros(1, batch_size, self.hidden_dim, dtype=x.data.dtype, device=x.data.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "class newsLSTM(nn.Module): \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size) -> None:\n",
    "        super(newsLSTM, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.hidden_size = hidden_size \n",
    "        \n",
    "        self.lstm = lstm(input_size=embed_size, hidden_dim=hidden_size)\n",
    "        \n",
    "        self.hidden2label = nn.Linear(hidden_size, 4)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropoutLayer = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, x_len):\n",
    "        embedded = self.encoder(x)\n",
    "        x_packed = nn.utils.rnn.pack_padded_sequence(embedded, x_len, batch_first=True, enforce_sorted=False)\n",
    "        output, h_t, c_t = self.lstm(x_packed)  # Pass the initial hidden state 'h' to the RNN        \n",
    "        hidden = self.dropoutLayer(h_t.squeeze())\n",
    "        \n",
    "        # Linear layer and softmax\n",
    "        label_space = self.hidden2label(hidden)\n",
    "        \n",
    "        return label_space\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the exact same news classification task performed by in the bidirectionalRNN subdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "# Convert to list to enable random splitting\n",
    "train_dataset = list(train_iter)\n",
    "\n",
    "#80-20 train-val split \n",
    "train_size = int(len(train_dataset) * 0.8)  \n",
    "val_size = len(train_dataset) - train_size  \n",
    "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "\n",
    "# Build vocab based on the train_data\n",
    "train_data_iter = (text for _, text in train_data)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data_iter), specials=[\"<unk>\"], max_tokens=VOCAB_SIZE)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " 'Wholesale prices jump in October by 1.7 percent, biggest increase &lt;b&gt;...&lt;/b&gt; Wholesale costs -- catapulted by more expensive energy and food -- soared last month by the largest amount in more than 14 years. With inflation at the producer level accelerating sharply after months of being ')"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2073, 1666, 4741, 0]"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['word', 'probably', 'unknown', 'gibberish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['policemen']"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_tokens([4999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    \n",
    "    # Sort the batch in the descending order\n",
    "    batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "        \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.int64)\n",
    "    \n",
    "    # Pad sequences\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "    \n",
    "    return label_list.to(device), text_list.to(device), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = 8, shuffle = True, collate_fn = collate_batch)\n",
    "val_loader = DataLoader(val_data, batch_size = 8, shuffle = False, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Inspect the shape of the input data\n",
    "input_data = batch[1]  # Assuming the input data is the first element of the batch\n",
    "input_shape = input_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 2, 2]),\n",
       " tensor([[1022,  170, 1831,    0, 4894,  247,  252,  942,   15,    2,  634, 1022,\n",
       "           703,    5, 1177,   90,    0,    0, 1775, 3155,   29, 3403,   19, 1495,\n",
       "             7,    2,  109,   16,    9, 1476, 4398, 4150,    0,    1,  304, 3155,\n",
       "          1875,   92,   16,    9,   87,    3,    2, 1183, 1321,   63,    2,  942,\n",
       "          1831,    7,    2, 1476,  736,  232, 1730,   56,    1,    1,    1],\n",
       "         [2235,  930,    0,    3,    0,  306,    3,  332,  220,   29, 3543, 1447,\n",
       "             3,  698,   15,    5,    0, 2327, 1084,   18,    0,    0,    4,   32,\n",
       "          1071,    0,  242,   10,    5, 1362,  379,  893,   18, 3537,   74,    3,\n",
       "           460, 1532,    8, 1900,  484,  841,    3,  332,   91, 3543,  666,    1,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 128,   84,  141,    1,  411,   12,    9,    0,    0,   63,  276,  857,\n",
       "            13, 4819,    3,    0,   14,  686,    1,  453,    3,  138,   15,   76,\n",
       "           128,   84,  265,  411,   12,    9,    0,   42,   89,    7, 1419,    5,\n",
       "             0, 1737,    4,    2,  343,   75,  822,   25,    3,   43,  216,    0,\n",
       "             6,  373,  276,  857, 1478,    1,    0,    0,    0,    0,    0],\n",
       "         [   0, 1107,    6, 2387,    0,    7,  996,   13,  103,   14,  103,   15,\n",
       "           233,  359,  246,  748,  996, 1587, 4027,    0,    7,    2,  358, 1846,\n",
       "            45,    2, 2414,    0, 3158,    6, 1890,   94,    0,  495, 3925,    3,\n",
       "             2,  941,  884,  193,   13,    0,   14,  684,    1,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [2014,  900,  155,    0,    0,    7,  243,   12,    9,  778, 1670,  155,\n",
       "           142,    0,  909,   11,    5,  192,   97,  337,   38,   81,  139,    7,\n",
       "             5, 1183,  302,    7,  243,    3,   61,    6,    2, 3944, 1897,  298,\n",
       "           817,   10,    2,    0,    0,  778, 1670,    1,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [  19,   46, 1015, 3569,    3,  928, 1217,    4, 1023,   68,   66,  905,\n",
       "            76,  128,    0, 3765,  273,  426,   96,    3,    8, 2356,    1,  426,\n",
       "             0,    2,   76,   16,    9,   68,  936,    7,    5, 1225, 1440,   24,\n",
       "            32,  491,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 968,  294,  369,    4,    0,   44,    4,  108,    3, 1285,  325,  968,\n",
       "           294,  369,   22,    4,  275,   44,    4,  108,    3, 1285,  325,    8,\n",
       "          4141,  257,    7,    5,  237,    4,    0,   21,    0,    0,  135,    3,\n",
       "           104,   28,   81,  229,   24,  815,   29, 3193,  677,    1,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [  22,  555,  779,  460, 1979,   80,  458,   16,    9, 1418,  164,  188,\n",
       "            16,   83,   38,    4, 3841,  235,  555,    8,    0,    1,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]),\n",
       " tensor([59, 48, 54, 45, 44, 39, 46, 22]))"
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 59])"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 689,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =  torch.ones(5, 50)\n",
    "a =  torch.ones(5, 50)\n",
    "a =  torch.ones(5, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 2, 2])\n",
      "torch.Size([8, 59])\n"
     ]
    }
   ],
   "source": [
    "print(batch[0])\n",
    "print(batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 50\n",
    "DROPOUT = 0.5\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "BIDIRECTIONAL = True\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_DIM = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = newsLSTM(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, loss_function, optim, epochs, device):\n",
    "    losses = [] #group losses for loss visualization \n",
    "    running_loss = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(\"Epoch %d / %d\" % (epoch+1, epochs))\n",
    "        print(\"-\"*10)\n",
    "    \n",
    "        for i, batch_data in enumerate(train_loader):\n",
    "            \n",
    "            model.train()\n",
    "            (y, x, x_size) = batch_data\n",
    "            y_pred = model(x, x_size.cpu())\n",
    "            loss = loss_function(y_pred, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            losses.append(loss)\n",
    "\n",
    "            if (i+1) % 1000 == 0:\n",
    "                print(\"Step: {}, average training loss over last 2000 steps: {:.4f}\".format(i+1, running_loss/1000))\n",
    "                running_loss = 0.0\n",
    "            \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, batch_data in enumerate(val_loader):\n",
    "                (y, x, x_size) = batch_data\n",
    "                y, x, x_size = y.to(device), x.to(device), x_size.to(device)\n",
    "                \n",
    "                y_pred = model(x, x_size.cpu())\n",
    "                loss = loss_function(y_pred, y)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(\"Epoch: {}, validation loss: {:.4f}\".format(epoch+1, val_loss/len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 50\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, torch.nn.functional.cross_entropy, optimizer, NUM_EPOCHS, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
