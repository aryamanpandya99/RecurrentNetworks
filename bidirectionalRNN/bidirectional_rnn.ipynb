{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Aryaman Pandya \n",
    "Sequential Machine Learning \n",
    "Building a Vanilla RNN \n",
    "Model and trainer implementation \n",
    "Following https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_agnews.ipynb\n",
    "implementation minus the memory unit for now \n",
    "'''\n",
    "import torch \n",
    "from torch.utils.data import random_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import plotly\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Class definition of Vanilla RNN \n",
    "class VanillaRNN(nn.Module): \n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_len, num_layers) -> None:\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.hidden_size = hidden_size \n",
    "        self.output_len = output_len \n",
    "        \n",
    "        self.rnn = nn.RNN(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                                batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.hidden2label = nn.Linear(2*hidden_size, 4)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropoutLayer = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, x_len):\n",
    "        embedded = self.encoder(x)\n",
    "        x_packed = nn.utils.rnn.pack_padded_sequence(embedded, x_len, batch_first=True, enforce_sorted=False)\n",
    "        output, hidden = self.rnn(x_packed)  # Pass the initial hidden state 'h' to the RNN\n",
    "        print(f\"hidden shape: {hidden.shape}\")\n",
    "        \n",
    "        hidden = self.dropoutLayer(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        \n",
    "        # Linear layer and softmax\n",
    "        label_space = self.hidden2label(hidden)\n",
    "        \n",
    "        return label_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "# Convert to list to enable random splitting\n",
    "train_dataset = list(train_iter)\n",
    "\n",
    "#80-20 train-val split \n",
    "train_size = int(len(train_dataset) * 0.8)  \n",
    "val_size = len(train_dataset) - train_size  \n",
    "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "\n",
    "# Build vocab based on the train_data\n",
    "train_data_iter = (text for _, text in train_data)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data_iter), specials=[\"<unk>\"], max_tokens=VOCAB_SIZE)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the vocabulary builds a dictionary of the most frequently observed words. This dictionary however, is pretty meaningless- it doesn't encode any semantic information about the words and is a simple string to integer mapping for further processing. In our nn model, the encoder (nn.Embedding) takes these integers and maps them to a higher dimensional space in which semantics and meaning is embedded. For example synonyms would be close to one another in vector space. nn.Embedding learns a look-up table that takes in indices of words and returns the corresponding embedding vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2117, 1573, 4679, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['word', 'probably', 'unknown', 'gibberish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shark']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_tokens([4999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    \n",
    "    # Sort the batch in the descending order\n",
    "    batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "        \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.int64)\n",
    "    \n",
    "    # Pad sequences\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "    \n",
    "    return label_list.to(device), text_list.to(device), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = 8, shuffle = True, collate_fn = collate_batch)\n",
    "val_loader = DataLoader(val_data, batch_size = 8, shuffle = False, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Inspect the shape of the input data\n",
    "input_data = batch[1]  # Assuming the input data is the first element of the batch\n",
    "input_shape = input_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 2, 0, 2, 1, 3, 3], device='cuda:0')\n",
      "tensor([[ 897,  906, 2231, 3582,    0,    0,  111,   80,    1,  258,  259,   41,\n",
      "          166,    1,    1,    1,   41,  165,    0,    0,  602, 1358,  352,    8,\n",
      "         4914,    0,  214,   93, 4874,    0,    4,  211, 3582,    0,    0,   80,\n",
      "            1,  258,  259, 1429,   12,    9,  118, 1740,    3,    0,    3,   70,\n",
      "          116,    7,    2,    0,    0,  927,    7, 1295,    3,  917,    1],\n",
      "        [   0,    3,    0,    4, 1034,    0, 3010,   36, 3010,  321,    0, 1332,\n",
      "           64,    1,    8,    0,    0,   38, 4142,    5, 1509, 1452,    4,  940,\n",
      "            2,  984,  436,  235,   12,    9,    0,  700,   66,    5,    0,   82,\n",
      "            0,    8,    0, 3363,    1,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [2783,    4, 3618,    0,   79, 3720,   12,    9,  637,  155,    4,  272,\n",
      "           59,    2,    0,  622,    8,   32, 1747,   29,    0, 1804,    3,  150,\n",
      "           84,    1,   23,   74,   13, 3814,   14,   15, 2783,  209,  102,   22,\n",
      "          419,    4,    5,  123,    4,  272,   59,    2,    0,    8, 1563,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 423,   84,   49,   16,    9,    7,    5,   16,  867,    0,   16,   13,\n",
      "           31,   14,   31,   15, 1049, 2049,    1,  278,  423,   26,  114,   49,\n",
      "           16,    9,    7,    5,  867,    0,   18,   48,  298,    4,  363,    4,\n",
      "            2,  395,  247,   19,   32, 2926, 3793,  284,   29, 1310,   68,   32,\n",
      "            0,    7, 2611,    1,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1379,    4,  273,   58, 1612,    7, 1817,    6,  329,    2, 4008,    6,\n",
      "            2, 3514,    0,  778,  296,   73,    4,  273,    2,  869,    6,   58,\n",
      "            4,   50,  697,    8, 3408,  286,   92,    7,    0,    6, 1336,   68,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    8,  731,    4,    0,    7,  204,    0,    2, 4384,    6, 3224,\n",
      "            0,    0,    2,    0,   39,    6,   52,    0,   26,   47,   68,    2,\n",
      "          124,    6, 2004, 1064,   71,  454,  435,    6, 2368,    1,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [2934, 1715, 1083,  426,  240, 1220, 1715,  218, 4561, 1087,   59,    4,\n",
      "         1193,  106,  437,  119,    0,    3,    0,    6,    5,    0,  459,    6,\n",
      "         2934, 1715,  270,    7,    2, 2263,    1,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0, 1764, 1510,    0,    2,   46,  354,    6,    2,  568,    6, 3312,\n",
      "          101,    0,    5, 1455, 1235,    3,   45,    0,  120, 4413,    0,    1,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(batch[0])\n",
    "print(batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 50\n",
    "DROPOUT = 0.5\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "BIDIRECTIONAL = True\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_DIM = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryaman.pandya/ml_accel/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model = VanillaRNN(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, NUM_LAYERS)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, loss_function, optim, epochs, device):\n",
    "    losses = [] #group losses for loss visualization \n",
    "    running_loss = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(\"Epoch %d / %d\" % (epoch+1, epochs))\n",
    "        print(\"-\"*10)\n",
    "    \n",
    "        for i, batch_data in enumerate(train_loader):\n",
    "            \n",
    "            model.train()\n",
    "            (y, x, x_size) = batch_data\n",
    "            #print(\"Labels: {}, data: {}, x_size.cpu(): {}\".format(batch_data[0], x.shape,x_size.cpu()))\n",
    "\n",
    "            logits = model(x, x_size.cpu())\n",
    "            #print(\"Target size: {}, pred_size: {}\".format(y.size(), logits.size()))\n",
    "            loss = loss_function(logits, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            losses.append(loss)\n",
    "\n",
    "            if (i+1) % 1000 == 0:\n",
    "                print(\"Step: {}, average training loss over last 2000 steps: {:.4f}\".format(i+1, running_loss/1000))\n",
    "                running_loss = 0.0\n",
    "            \n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, batch_data in enumerate(val_loader):\n",
    "                (y, x, x_size) = batch_data\n",
    "                y, x, x_size = y.to(device), x.to(device), x_size.to(device)\n",
    "                \n",
    "                logits = model(x, x_size.cpu())\n",
    "                loss = loss_function(logits, y)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(\"Epoch: {}, validation loss: {:.4f}\".format(epoch+1, val_loss/len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 50\n",
      "----------\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n",
      "torch.Size([4, 8, 256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model, train_loader, val_loader, torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mcross_entropy, optimizer, NUM_EPOCHS, DEVICE)\n",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, loss_function, optim, epochs, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m (y, x, x_size) \u001b[39m=\u001b[39m batch_data\n\u001b[1;32m     13\u001b[0m \u001b[39m#print(\"Labels: {}, data: {}, x_size.cpu(): {}\".format(batch_data[0], x.shape,x_size.cpu()))\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m logits \u001b[39m=\u001b[39m model(x, x_size\u001b[39m.\u001b[39;49mcpu())\n\u001b[1;32m     16\u001b[0m \u001b[39m#print(\"Target size: {}, pred_size: {}\".format(y.size(), logits.size()))\u001b[39;00m\n\u001b[1;32m     17\u001b[0m loss \u001b[39m=\u001b[39m loss_function(logits, y)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m, in \u001b[0;36mVanillaRNN.forward\u001b[0;34m(self, x, x_len)\u001b[0m\n\u001b[1;32m     36\u001b[0m embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x)\n\u001b[1;32m     37\u001b[0m x_packed \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpack_padded_sequence(embedded, x_len, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, enforce_sorted\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 38\u001b[0m output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x_packed)  \u001b[39m# Pass the initial hidden state 'h' to the RNN\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mprint\u001b[39m(hidden\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     41\u001b[0m hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropoutLayer(torch\u001b[39m.\u001b[39mcat((hidden[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m,:,:], hidden[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:,:]), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/torch/nn/modules/rnn.py:562\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    561\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRNN_TANH\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 562\u001b[0m         result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mrnn_tanh(\u001b[39minput\u001b[39;49m, batch_sizes, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    563\u001b[0m                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m    564\u001b[0m                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional)\n\u001b[1;32m    565\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    566\u001b[0m         result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mrnn_relu(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    567\u001b[0m                               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining,\n\u001b[1;32m    568\u001b[0m                               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, torch.nn.functional.cross_entropy, optimizer, NUM_EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
