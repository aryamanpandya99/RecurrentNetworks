{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Aryaman Pandya \n",
    "Sequential Machine Learning \n",
    "Building a Vanilla RNN \n",
    "Model and trainer implementation \n",
    "Following https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_agnews.ipynb\n",
    "implementation minus the memory unit for now \n",
    "'''\n",
    "import torch \n",
    "from torch.utils.data import random_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import plotly\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Class definition of Vanilla RNN \n",
    "class VanillaRNN(nn.Module): \n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_len, num_layers) -> None:\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.hidden_size = hidden_size \n",
    "        self.output_len = output_len \n",
    "        \n",
    "        self.rnn = nn.RNN(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                                batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.hidden2label = nn.Linear(2*hidden_size, 4)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropoutLayer = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, x_len):\n",
    "        embedded = self.encoder(x)\n",
    "        x_packed = nn.utils.rnn.pack_padded_sequence(embedded, x_len, batch_first=True, enforce_sorted=False)\n",
    "        output, hidden = self.rnn(x_packed)  # Pass the initial hidden state 'h' to the RNN\n",
    "        \n",
    "        \n",
    "        hidden = self.dropoutLayer(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        \n",
    "        # Linear layer and softmax\n",
    "        label_space = self.hidden2label(hidden)\n",
    "        \n",
    "        return label_space\n",
    "\n",
    "    def init_h(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "# Convert to list to enable random splitting\n",
    "train_dataset = list(train_iter)\n",
    "\n",
    "#80-20 train-val split \n",
    "train_size = int(len(train_dataset) * 0.8)  \n",
    "val_size = len(train_dataset) - train_size  \n",
    "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "\n",
    "# Build vocab based on the train_data\n",
    "train_data_iter = (text for _, text in train_data)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data_iter), specials=[\"<unk>\"], max_tokens=VOCAB_SIZE)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the vocabulary builds a dictionary of the most frequently observed words. This dictionary however, is pretty meaningless- it doesn't encode any semantic information about the words and is a simple string to integer mapping for further processing. In our nn model, the encoder (nn.Embedding) takes these integers and maps them to a higher dimensional space in which semantics and meaning is embedded. For example synonyms would be close to one another in vector space. nn.Embedding learns a look-up table that takes in indices of words and returns the corresponding embedding vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2033, 1611, 4843, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['word', 'probably', 'unknown', 'gibberish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['proof']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_tokens([4999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    \n",
    "    # Sort the batch in the descending order\n",
    "    batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "        \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.int64)\n",
    "    \n",
    "    # Pad sequences\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "    \n",
    "    return label_list.to(device), text_list.to(device), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = 8, shuffle = True, collate_fn = collate_batch)\n",
    "val_loader = DataLoader(val_data, batch_size = 8, shuffle = False, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Inspect the shape of the input data\n",
    "input_data = batch[1]  # Assuming the input data is the first element of the batch\n",
    "input_shape = input_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 3, 1, 3, 0, 2, 1], device='cuda:0')\n",
      "tensor([[1081, 1065,   10,  528,  776,    3, 1136, 1168,   23,   73,   13,   27,\n",
      "           14,   15,    2, 1081,  794,  108,  106,  275,   10,   60,    3,  662,\n",
      "           24,    5,  335,    7, 1685,  193,    3,   45,    2,    0, 1136,   35,\n",
      "         3623,  135,    4,    5,  986,    0,  691,   24,    5,  431,    7, 1658,\n",
      "            0,    0,   64,    1,   41,    5,  261,  258,    1,  259,    1,   27,\n",
      "            1,  312,    1,  311,   80,    0,    1,  140,  313, 4405,    1,  513,\n",
      "          260,    1],\n",
      "        [   0,   16,    9, 1900, 2486,    0,    6, 4282,  773, 2507,  132, 4021,\n",
      "            0,    0,    0,   26,  564, 1172,   16,   83, 1216, 2907,   68,    2,\n",
      "         1012,    0,  784, 4282,   24,  256, 3851,    3,    0,  988,    1,    0,\n",
      "            3,  166,  564, 1247,    0,   16,    9,    0,    0,   69,   60,   20,\n",
      "          256,  537,   20,  697,  806,    0,    3,  104,   49, 4912,    1,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   0,    0,    0, 2290,   77,   34,  239,    0, 1513,  920,   30, 1678,\n",
      "         1004,   69,   85,   63,   77,   38, 1012,    0,   17, 4091,   38,  196,\n",
      "           97,  359,    1, 1513,   26,   25,  966,    4,  282, 1166,   18,    2,\n",
      "         1004,   90,   25,  134,    5, 1322,   18,   77,   10,  160, 1678,  566,\n",
      "            1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [4607, 4002,   66,  603,  877,   15,    0,    5,  399,    0,    0,    6,\n",
      "            2,   89,    3, 4635, 4607, 1697,   61,    6,  545,   12,    9,    0,\n",
      "         1743,   65,  115,    3,    0,    5, 1414,   43,    2, 1139,   11,   32,\n",
      "            0,  234,    6,    2,   70,    8, 2095,  820,    0,   12,    9,  146,\n",
      "           11,    2,  142,  617,    7,    5,  118,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 930,    1, 1421, 4366, 4938,  715,  236,   11, 1742,   13,   27,   14,\n",
      "           27,   15,    5,  236,    4, 4319,    0,    0, 4938,  715, 3588,   29,\n",
      "         1742,    8, 4431,   24,   68,    0,   21,  242,    4,   37, 1062,   10,\n",
      "           60,   24,    0,  329, 2464,  524,    3,    0,    8, 2785,   26,    1,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 248,  143,    3, 1960, 1246,   19,  645,  409,    0,    7,  190,   16,\n",
      "            9, 3027,   13,  105,   14,  105,   15,  248,  100,   79,  143,    8,\n",
      "         1960, 1156,    7, 3027,  190,   90,  536, 3612,   43,   47,    0,    3,\n",
      "         1562,    5, 4519,   66,    5,  125,    8, 4503, 4572, 1919,   10,    5,\n",
      "            0, 1019,    3,  147,   26,    1,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 128,    0, 3512,  633,  648,   76,  128,   84,  538,    0, 1618,    5,\n",
      "         4792,  648,    7,    2,   36, 3512,  633,  527,  401,   12,    9,  349,\n",
      "           21,  395,    8,  813, 2467,    1,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 192,  295,    3,  490,  223, 1332, 4419,   13,   31,   14,   31,   15,\n",
      "           29,    2, 2548, 4329,    0,   16,    9,  158,  386, 2635,    2,    0,\n",
      "          443,   69,  459,    3,    2,  490,    8,  192,  295,   39,   82, 1112,\n",
      "            4,   52,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(batch[0])\n",
    "print(batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 50\n",
    "DROPOUT = 0.5\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "BIDIRECTIONAL = True\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_DIM = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryaman.pandya/ml_accel/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model = VanillaRNN(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, NUM_LAYERS)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, loss_function, optim, epochs, device):\n",
    "    losses = [] #group losses for loss visualization \n",
    "    running_loss = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(\"Epoch %d / %d\" % (epoch+1, epochs))\n",
    "        print(\"-\"*10)\n",
    "    \n",
    "        for i, batch_data in enumerate(train_loader):\n",
    "            \n",
    "            model.train()\n",
    "            (y, x, x_size) = batch_data\n",
    "            #print(\"Labels: {}, data: {}, x_size.cpu(): {}\".format(batch_data[0], x.shape,x_size.cpu()))\n",
    "\n",
    "            logits = model(x, x_size.cpu())\n",
    "            #print(\"Target size: {}, pred_size: {}\".format(y.size(), logits.size()))\n",
    "            loss = loss_function(logits, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            losses.append(loss)\n",
    "\n",
    "            if (i+1) % 1000 == 0:\n",
    "                print(\"Step: {}, average training loss over last 2000 steps: {:.4f}\".format(i+1, running_loss/1000))\n",
    "                running_loss = 0.0\n",
    "            \n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, batch_data in enumerate(val_loader):\n",
    "                (y, x, x_size) = batch_data\n",
    "                y, x, x_size = y.to(device), x.to(device), x_size.to(device)\n",
    "                \n",
    "                logits = model(x, x_size.cpu())\n",
    "                loss = loss_function(logits, y)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(\"Epoch: {}, validation loss: {:.4f}\".format(epoch+1, val_loss/len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 1.4264\n",
      "Step: 2000, average training loss over last 2000 steps: 1.2945\n",
      "Step: 3000, average training loss over last 2000 steps: 1.1774\n",
      "Step: 4000, average training loss over last 2000 steps: 1.0743\n",
      "Step: 5000, average training loss over last 2000 steps: 0.9865\n",
      "Step: 6000, average training loss over last 2000 steps: 0.9935\n",
      "Step: 7000, average training loss over last 2000 steps: 0.8899\n",
      "Step: 8000, average training loss over last 2000 steps: 0.8938\n",
      "Step: 9000, average training loss over last 2000 steps: 0.8132\n",
      "Step: 10000, average training loss over last 2000 steps: 0.7535\n",
      "Step: 11000, average training loss over last 2000 steps: 0.7491\n",
      "Step: 12000, average training loss over last 2000 steps: 0.7968\n",
      "Epoch: 1, validation loss: 0.6686\n",
      "Epoch 2 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.7297\n",
      "Step: 2000, average training loss over last 2000 steps: 0.6924\n",
      "Step: 3000, average training loss over last 2000 steps: 0.7674\n",
      "Step: 4000, average training loss over last 2000 steps: 0.8285\n",
      "Step: 5000, average training loss over last 2000 steps: 0.6877\n",
      "Step: 6000, average training loss over last 2000 steps: 0.6436\n",
      "Step: 7000, average training loss over last 2000 steps: 0.8407\n",
      "Step: 8000, average training loss over last 2000 steps: 0.7415\n",
      "Step: 9000, average training loss over last 2000 steps: 0.6557\n",
      "Step: 10000, average training loss over last 2000 steps: 0.6304\n",
      "Step: 11000, average training loss over last 2000 steps: 0.6074\n",
      "Step: 12000, average training loss over last 2000 steps: 0.6225\n",
      "Epoch: 2, validation loss: 0.6073\n",
      "Epoch 3 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.5726\n",
      "Step: 2000, average training loss over last 2000 steps: 0.5847\n",
      "Step: 3000, average training loss over last 2000 steps: 0.6288\n",
      "Step: 4000, average training loss over last 2000 steps: 0.6035\n",
      "Step: 5000, average training loss over last 2000 steps: 0.6751\n",
      "Step: 6000, average training loss over last 2000 steps: 0.6020\n",
      "Step: 7000, average training loss over last 2000 steps: 0.6381\n",
      "Step: 8000, average training loss over last 2000 steps: 0.6659\n",
      "Step: 9000, average training loss over last 2000 steps: 0.6170\n",
      "Step: 10000, average training loss over last 2000 steps: 0.6197\n",
      "Step: 11000, average training loss over last 2000 steps: 0.5913\n",
      "Step: 12000, average training loss over last 2000 steps: 0.5806\n",
      "Epoch: 3, validation loss: 0.7395\n",
      "Epoch 4 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.6938\n",
      "Step: 2000, average training loss over last 2000 steps: 0.6548\n",
      "Step: 3000, average training loss over last 2000 steps: 0.6322\n",
      "Step: 4000, average training loss over last 2000 steps: 0.6819\n",
      "Step: 5000, average training loss over last 2000 steps: 0.7461\n",
      "Step: 6000, average training loss over last 2000 steps: 0.6838\n",
      "Step: 7000, average training loss over last 2000 steps: 0.6457\n",
      "Step: 8000, average training loss over last 2000 steps: 0.6118\n",
      "Step: 9000, average training loss over last 2000 steps: 0.6270\n",
      "Step: 10000, average training loss over last 2000 steps: 0.6105\n",
      "Step: 11000, average training loss over last 2000 steps: 0.6392\n",
      "Step: 12000, average training loss over last 2000 steps: 0.6076\n",
      "Epoch: 4, validation loss: 0.6323\n",
      "Epoch 5 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.6064\n",
      "Step: 2000, average training loss over last 2000 steps: 0.7013\n",
      "Step: 3000, average training loss over last 2000 steps: 0.6383\n",
      "Step: 4000, average training loss over last 2000 steps: 0.5480\n",
      "Step: 5000, average training loss over last 2000 steps: 0.5361\n",
      "Step: 6000, average training loss over last 2000 steps: 0.5542\n",
      "Step: 7000, average training loss over last 2000 steps: 0.5766\n",
      "Step: 8000, average training loss over last 2000 steps: 0.5526\n",
      "Step: 9000, average training loss over last 2000 steps: 0.5434\n",
      "Step: 10000, average training loss over last 2000 steps: 0.4965\n",
      "Step: 11000, average training loss over last 2000 steps: 0.4892\n",
      "Step: 12000, average training loss over last 2000 steps: 0.5112\n",
      "Epoch: 5, validation loss: 0.4736\n",
      "Epoch 6 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.4957\n",
      "Step: 2000, average training loss over last 2000 steps: 0.5206\n",
      "Step: 3000, average training loss over last 2000 steps: 0.5109\n",
      "Step: 4000, average training loss over last 2000 steps: 0.5032\n",
      "Step: 5000, average training loss over last 2000 steps: 0.5368\n",
      "Step: 6000, average training loss over last 2000 steps: 0.5012\n",
      "Step: 7000, average training loss over last 2000 steps: 0.5286\n",
      "Step: 8000, average training loss over last 2000 steps: 0.5141\n",
      "Step: 9000, average training loss over last 2000 steps: 0.5294\n",
      "Step: 10000, average training loss over last 2000 steps: 0.5030\n",
      "Step: 11000, average training loss over last 2000 steps: 0.5271\n",
      "Step: 12000, average training loss over last 2000 steps: 0.5151\n",
      "Epoch: 6, validation loss: 0.4768\n",
      "Epoch 7 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.5072\n",
      "Step: 2000, average training loss over last 2000 steps: 0.5076\n",
      "Step: 3000, average training loss over last 2000 steps: 0.4869\n",
      "Step: 4000, average training loss over last 2000 steps: 0.4827\n",
      "Step: 5000, average training loss over last 2000 steps: 0.4833\n",
      "Step: 6000, average training loss over last 2000 steps: 0.5518\n",
      "Step: 7000, average training loss over last 2000 steps: 0.5427\n",
      "Step: 8000, average training loss over last 2000 steps: 0.5084\n",
      "Step: 9000, average training loss over last 2000 steps: 0.5400\n",
      "Step: 10000, average training loss over last 2000 steps: 0.5280\n",
      "Step: 11000, average training loss over last 2000 steps: 0.5988\n",
      "Step: 12000, average training loss over last 2000 steps: 0.5520\n",
      "Epoch: 7, validation loss: 0.4841\n",
      "Epoch 8 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.5075\n",
      "Step: 2000, average training loss over last 2000 steps: 0.4999\n",
      "Step: 3000, average training loss over last 2000 steps: 0.4838\n",
      "Step: 4000, average training loss over last 2000 steps: 0.5004\n",
      "Step: 5000, average training loss over last 2000 steps: 0.4903\n",
      "Step: 6000, average training loss over last 2000 steps: 0.5181\n",
      "Step: 7000, average training loss over last 2000 steps: 0.4870\n",
      "Step: 8000, average training loss over last 2000 steps: 0.4740\n",
      "Step: 9000, average training loss over last 2000 steps: 0.4967\n",
      "Step: 10000, average training loss over last 2000 steps: 0.5087\n",
      "Step: 11000, average training loss over last 2000 steps: 0.4845\n",
      "Step: 12000, average training loss over last 2000 steps: 0.4664\n",
      "Epoch: 8, validation loss: 0.4554\n",
      "Epoch 9 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.4919\n",
      "Step: 2000, average training loss over last 2000 steps: 0.5008\n",
      "Step: 3000, average training loss over last 2000 steps: 0.4758\n",
      "Step: 4000, average training loss over last 2000 steps: 0.5292\n",
      "Step: 5000, average training loss over last 2000 steps: 0.5143\n",
      "Step: 6000, average training loss over last 2000 steps: 0.5698\n",
      "Step: 7000, average training loss over last 2000 steps: 0.5274\n",
      "Step: 8000, average training loss over last 2000 steps: 0.5349\n",
      "Step: 9000, average training loss over last 2000 steps: 0.5415\n",
      "Step: 10000, average training loss over last 2000 steps: 0.5341\n",
      "Step: 11000, average training loss over last 2000 steps: 0.5002\n",
      "Step: 12000, average training loss over last 2000 steps: 0.4644\n",
      "Epoch: 9, validation loss: 0.4907\n",
      "Epoch 10 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.4822\n",
      "Step: 2000, average training loss over last 2000 steps: 0.4940\n",
      "Step: 3000, average training loss over last 2000 steps: 0.5387\n",
      "Step: 4000, average training loss over last 2000 steps: 0.4618\n",
      "Step: 5000, average training loss over last 2000 steps: 0.4837\n",
      "Step: 6000, average training loss over last 2000 steps: 0.4837\n",
      "Step: 7000, average training loss over last 2000 steps: 0.4809\n",
      "Step: 8000, average training loss over last 2000 steps: 0.4931\n",
      "Step: 9000, average training loss over last 2000 steps: 0.4954\n",
      "Step: 10000, average training loss over last 2000 steps: 0.4914\n",
      "Step: 11000, average training loss over last 2000 steps: 0.4934\n",
      "Step: 12000, average training loss over last 2000 steps: 0.4575\n",
      "Epoch: 10, validation loss: 0.4815\n",
      "Epoch 11 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.4510\n",
      "Step: 2000, average training loss over last 2000 steps: 0.4946\n",
      "Step: 3000, average training loss over last 2000 steps: 0.5184\n",
      "Step: 4000, average training loss over last 2000 steps: 0.4810\n",
      "Step: 5000, average training loss over last 2000 steps: 0.4649\n",
      "Step: 6000, average training loss over last 2000 steps: 0.4592\n",
      "Step: 7000, average training loss over last 2000 steps: 0.4866\n",
      "Step: 8000, average training loss over last 2000 steps: 0.4565\n",
      "Step: 9000, average training loss over last 2000 steps: 0.4500\n",
      "Step: 10000, average training loss over last 2000 steps: 0.4858\n",
      "Step: 11000, average training loss over last 2000 steps: 0.4725\n",
      "Step: 12000, average training loss over last 2000 steps: 0.4805\n",
      "Epoch: 11, validation loss: 0.4551\n",
      "Epoch 12 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.4592\n",
      "Step: 2000, average training loss over last 2000 steps: 0.4678\n",
      "Step: 3000, average training loss over last 2000 steps: 0.4801\n",
      "Step: 4000, average training loss over last 2000 steps: 0.4891\n",
      "Step: 5000, average training loss over last 2000 steps: 0.4856\n",
      "Step: 6000, average training loss over last 2000 steps: 0.4539\n",
      "Step: 7000, average training loss over last 2000 steps: 0.4669\n",
      "Step: 8000, average training loss over last 2000 steps: 0.5088\n",
      "Step: 9000, average training loss over last 2000 steps: 0.4714\n",
      "Step: 10000, average training loss over last 2000 steps: 0.5125\n",
      "Step: 11000, average training loss over last 2000 steps: 0.4665\n",
      "Step: 12000, average training loss over last 2000 steps: 0.4942\n",
      "Epoch: 12, validation loss: 0.4665\n",
      "Epoch 13 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.4856\n",
      "Step: 2000, average training loss over last 2000 steps: 0.4791\n",
      "Step: 3000, average training loss over last 2000 steps: 0.4938\n",
      "Step: 4000, average training loss over last 2000 steps: 0.4669\n",
      "Step: 5000, average training loss over last 2000 steps: 0.4891\n",
      "Step: 6000, average training loss over last 2000 steps: 0.4817\n",
      "Step: 7000, average training loss over last 2000 steps: 0.4987\n",
      "Step: 8000, average training loss over last 2000 steps: 0.4717\n",
      "Step: 9000, average training loss over last 2000 steps: 0.4639\n",
      "Step: 10000, average training loss over last 2000 steps: 0.4699\n",
      "Step: 11000, average training loss over last 2000 steps: 0.4999\n",
      "Step: 12000, average training loss over last 2000 steps: 0.4903\n",
      "Epoch: 13, validation loss: 0.4989\n",
      "Epoch 14 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.4747\n",
      "Step: 2000, average training loss over last 2000 steps: 0.5178\n",
      "Step: 3000, average training loss over last 2000 steps: 0.5424\n",
      "Step: 4000, average training loss over last 2000 steps: 0.5067\n",
      "Step: 5000, average training loss over last 2000 steps: 0.4938\n",
      "Step: 6000, average training loss over last 2000 steps: 0.4705\n",
      "Step: 7000, average training loss over last 2000 steps: 0.4553\n",
      "Step: 8000, average training loss over last 2000 steps: 0.4570\n",
      "Step: 9000, average training loss over last 2000 steps: 0.4550\n",
      "Step: 10000, average training loss over last 2000 steps: 0.4432\n",
      "Step: 11000, average training loss over last 2000 steps: 0.4725\n",
      "Step: 12000, average training loss over last 2000 steps: 0.4854\n",
      "Epoch: 14, validation loss: 0.4534\n",
      "Epoch 15 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.4526\n",
      "Step: 2000, average training loss over last 2000 steps: 0.4715\n",
      "Step: 3000, average training loss over last 2000 steps: 0.4581\n",
      "Step: 4000, average training loss over last 2000 steps: 0.4881\n",
      "Step: 5000, average training loss over last 2000 steps: 0.4511\n",
      "Step: 6000, average training loss over last 2000 steps: 0.4790\n",
      "Step: 7000, average training loss over last 2000 steps: 0.4675\n",
      "Step: 8000, average training loss over last 2000 steps: 0.5196\n",
      "Step: 9000, average training loss over last 2000 steps: 0.4720\n",
      "Step: 10000, average training loss over last 2000 steps: 0.4918\n",
      "Step: 11000, average training loss over last 2000 steps: 0.4944\n",
      "Step: 12000, average training loss over last 2000 steps: 0.5274\n",
      "Epoch: 15, validation loss: 0.4776\n",
      "Epoch 16 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.4757\n",
      "Step: 2000, average training loss over last 2000 steps: 0.4791\n",
      "Step: 3000, average training loss over last 2000 steps: 0.4610\n",
      "Step: 4000, average training loss over last 2000 steps: 0.4533\n",
      "Step: 5000, average training loss over last 2000 steps: 0.4737\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, torch.nn.functional.cross_entropy, optimizer, NUM_EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "ml_accel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
